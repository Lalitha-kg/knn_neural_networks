{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "knn_neural_nw.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBk7ISNz4N0P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nEdPGD34vuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('wine_dataset.csv')\n",
        "# print(df)\n",
        "a = pd.get_dummies(df['Wine'])\n",
        "df = pd.concat([df,a],axis=1)\n",
        "X = df.drop([1, 2,3,'Wine'], axis = 1)\n",
        "y = df[[1,2,3]].values\n",
        "X_train, X_test, Y_train,Y_test = train_test_split(X, y, test_size=0.20,)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "# Y_test,test_"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1Ajln9B41Ox",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def forward_prop(model,a0):\n",
        "    \n",
        "    # Load parameters from model\n",
        "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3']\n",
        "    # Do the first Linear step \n",
        "    # Z1 is the input layer x times the dot product of the weights + bias b\n",
        "    z1 = a0.dot(W1) + b1\n",
        "    \n",
        "    # Put it through the first activation function\n",
        "    a1 = np.tanh(z1)\n",
        "    \n",
        "    # Second linear step\n",
        "    z2 = a1.dot(W2) + b2\n",
        "    \n",
        "    # Second activation function\n",
        "    a2 = np.tanh(z2)\n",
        "    \n",
        "    #Third linear step\n",
        "    z3 = a2.dot(W3) + b3\n",
        "    \n",
        "    #For the Third linear activation function we use the softmax function, either the sigmoid of softmax should be used for the last layer\n",
        "    a3 = softmax(z3)\n",
        "    \n",
        "    #Store all results in these values\n",
        "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3}\n",
        "    return cache"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz3v3sqs5DK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(z):\n",
        "    #Calculate exponent term first\n",
        "    exp_scores = np.exp(z)\n",
        "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUuRAbe05I8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def backward_prop(model,cache,y):\n",
        "\n",
        "    # Load parameters from model\n",
        "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'],model['W3'],model['b3']\n",
        "    \n",
        "    # Load forward propagation results\n",
        "    a0,a1, a2,a3 = cache['a0'],cache['a1'],cache['a2'],cache['a3']\n",
        "    \n",
        "    # Get number of samples\n",
        "    m = y.shape[0]\n",
        "    \n",
        "    # Calculate loss derivative with respect to output\n",
        "    dz3 = loss_derivative(y=y,y_hat=a3)\n",
        "\n",
        "    # Calculate loss derivative with respect to second layer weights\n",
        "    dW3 = 1/m*(a2.T).dot(dz3) #dW2 = 1/m*(a1.T).dot(dz2) \n",
        "    \n",
        "    # Calculate loss derivative with respect to second layer bias\n",
        "    db3 = 1/m*np.sum(dz3, axis=0)\n",
        "    \n",
        "    # Calculate loss derivative with respect to first layer\n",
        "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
        "    \n",
        "    # Calculate loss derivative with respect to first layer weights\n",
        "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
        "    \n",
        "    # Calculate loss derivative with respect to first layer bias\n",
        "    db2 = 1/m*np.sum(dz2, axis=0)\n",
        "    \n",
        "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
        "    \n",
        "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
        "    \n",
        "    db1 = 1/m*np.sum(dz1,axis=0)\n",
        "    \n",
        "    # Store gradients\n",
        "    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
        "    return grads"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if-gwqyG5aIa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "df405748-0e6c-420e-c5c5-ae206fce638f"
      },
      "source": [
        "def softmax_loss(y,y_hat):\n",
        "    # Clipping value\n",
        "    minval = 0.000000000001\n",
        "    # Number of samples\n",
        "    m = y.shape[0]\n",
        "    # Loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula\n",
        "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
        "    return loss\n",
        "\n",
        "def loss_derivative(y,y_hat):\n",
        "    return (y_hat-y)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return (1 - np.power(x, 2))\n",
        "#TRAINING PHASE\n",
        "def initialize_parameters(nn_input_dim,nn_hdim,nn_output_dim):\n",
        "    # First layer weights\n",
        "    W1 = 2 *np.random.randn(nn_input_dim, nn_hdim) - 1\n",
        "    \n",
        "    # First layer bias\n",
        "    b1 = np.zeros((1, nn_hdim))\n",
        "    \n",
        "    # Second layer weights\n",
        "    W2 = 2 * np.random.randn(nn_hdim, nn_hdim) - 1\n",
        "    \n",
        "    # Second layer bias\n",
        "    b2 = np.zeros((1, nn_hdim))\n",
        "    W3 = 2 * np.random.rand(nn_hdim, nn_output_dim) - 1\n",
        "    b3 = np.zeros((1,nn_output_dim))\n",
        "    \n",
        "    \n",
        "    # Package and return model\n",
        "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3}\n",
        "    return model\n",
        "def update_parameters(model,grads,learning_rate):\n",
        "    # Load parameters\n",
        "    W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
        "    \n",
        "    # Update parameters\n",
        "    W1 -= learning_rate * grads['dW1']\n",
        "    b1 -= learning_rate * grads['db1']\n",
        "    W2 -= learning_rate * grads['dW2']\n",
        "    b2 -= learning_rate * grads['db2']\n",
        "    W3 -= learning_rate * grads['dW3']\n",
        "    b3 -= learning_rate * grads['db3']\n",
        "    \n",
        "    # Store and return parameters\n",
        "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3':W3,'b3':b3}\n",
        "    return model\n",
        "\n",
        "def predict(model, x):\n",
        "    # Do forward pass\n",
        "    c = forward_prop(model,x)\n",
        "    #get y_hat\n",
        "    y_hat = np.argmax(c['a3'], axis=1)\n",
        "    return y_hat\n",
        "def calc_accuracy(model,x,y):\n",
        "    # Get total number of examples\n",
        "    m = y.shape[0]\n",
        "    # Do a prediction with the model\n",
        "    pred = predict(model,x)\n",
        "    # Ensure prediction and truth vector y have the same shape\n",
        "    pred = pred.reshape(y.shape)\n",
        "    # Calculate the number of wrong examples\n",
        "    error = np.sum(np.abs(pred-y))\n",
        "    # Calculate accuracy\n",
        "    return (m - error)/m * 100\n",
        "losses = []\n",
        "\n",
        "def train(model,X_,y_,learning_rate, epochs, print_loss=False):\n",
        "    # Gradient descent. Loop over epochs\n",
        "    for i in range(0, epochs):\n",
        "\n",
        "        # Forward propagation\n",
        "        cache = forward_prop(model,X_)\n",
        "\n",
        "        # Backpropagation\n",
        "        grads = backward_prop(model,cache,y_)\n",
        "        \n",
        "        # Gradient descent parameter update\n",
        "        # Assign new parameters to the model\n",
        "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
        "    \n",
        "        # Pring loss & accuracy every 100 iterations\n",
        "        if print_loss and i % 100 == 0:\n",
        "            a3 = cache['a3']\n",
        "            print('Loss after iteration',i,':',softmax_loss(y_,a3))\n",
        "            y_hat = predict(model,X_)\n",
        "            y_true = y_.argmax(axis=1)\n",
        "            print('Accuracy after iteration',i,':',accuracy_score(y_pred=y_hat,y_true=y_true)*100,'%')\n",
        "            losses.append(accuracy_score(y_pred=y_hat,y_true=y_true)*100)\n",
        "    return model\n",
        "model = initialize_parameters(nn_input_dim=13, nn_hdim= 5, nn_output_dim= 3)\n",
        "model = train(model,X_train,Y_train,learning_rate=0.07,epochs=4500,print_loss=True)\n",
        "plt.plot(losses)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss after iteration 0 : 0.8841405024975727\n",
            "Accuracy after iteration 0 : 55.633802816901415 %\n",
            "Loss after iteration 100 : 0.533978477726491\n",
            "Accuracy after iteration 100 : 72.53521126760563 %\n",
            "Loss after iteration 200 : 0.4470042974766323\n",
            "Accuracy after iteration 200 : 80.28169014084507 %\n",
            "Loss after iteration 300 : 0.35944515832579144\n",
            "Accuracy after iteration 300 : 88.02816901408451 %\n",
            "Loss after iteration 400 : 0.2711904013996319\n",
            "Accuracy after iteration 400 : 92.25352112676056 %\n",
            "Loss after iteration 500 : 0.21201632300732753\n",
            "Accuracy after iteration 500 : 93.66197183098592 %\n",
            "Loss after iteration 600 : 0.1862687730873335\n",
            "Accuracy after iteration 600 : 94.36619718309859 %\n",
            "Loss after iteration 700 : 0.15875210192288097\n",
            "Accuracy after iteration 700 : 95.07042253521126 %\n",
            "Loss after iteration 800 : 0.13782078802023215\n",
            "Accuracy after iteration 800 : 95.77464788732394 %\n",
            "Loss after iteration 900 : 0.12155466133106735\n",
            "Accuracy after iteration 900 : 95.77464788732394 %\n",
            "Loss after iteration 1000 : 0.10466760301093003\n",
            "Accuracy after iteration 1000 : 97.88732394366197 %\n",
            "Loss after iteration 1100 : 0.09718033440645663\n",
            "Accuracy after iteration 1100 : 98.59154929577466 %\n",
            "Loss after iteration 1200 : 0.09050493213272484\n",
            "Accuracy after iteration 1200 : 98.59154929577466 %\n",
            "Loss after iteration 1300 : 0.0847811297277332\n",
            "Accuracy after iteration 1300 : 98.59154929577466 %\n",
            "Loss after iteration 1400 : 0.07993737825374173\n",
            "Accuracy after iteration 1400 : 98.59154929577466 %\n",
            "Loss after iteration 1500 : 0.07507776080918005\n",
            "Accuracy after iteration 1500 : 98.59154929577466 %\n",
            "Loss after iteration 1600 : 0.06866669221492783\n",
            "Accuracy after iteration 1600 : 98.59154929577466 %\n",
            "Loss after iteration 1700 : 0.06281396863212936\n",
            "Accuracy after iteration 1700 : 98.59154929577466 %\n",
            "Loss after iteration 1800 : 0.057739833166104614\n",
            "Accuracy after iteration 1800 : 98.59154929577466 %\n",
            "Loss after iteration 1900 : 0.05446550644408958\n",
            "Accuracy after iteration 1900 : 98.59154929577466 %\n",
            "Loss after iteration 2000 : 0.05167978764343605\n",
            "Accuracy after iteration 2000 : 98.59154929577466 %\n",
            "Loss after iteration 2100 : 0.04919627272073129\n",
            "Accuracy after iteration 2100 : 98.59154929577466 %\n",
            "Loss after iteration 2200 : 0.046809831745801965\n",
            "Accuracy after iteration 2200 : 98.59154929577466 %\n",
            "Loss after iteration 2300 : 0.044231107053558004\n",
            "Accuracy after iteration 2300 : 98.59154929577466 %\n",
            "Loss after iteration 2400 : 0.042536929340654714\n",
            "Accuracy after iteration 2400 : 98.59154929577466 %\n",
            "Loss after iteration 2500 : 0.04134999656856928\n",
            "Accuracy after iteration 2500 : 98.59154929577466 %\n",
            "Loss after iteration 2600 : 0.040333843541481645\n",
            "Accuracy after iteration 2600 : 98.59154929577466 %\n",
            "Loss after iteration 2700 : 0.03942970381978774\n",
            "Accuracy after iteration 2700 : 98.59154929577466 %\n",
            "Loss after iteration 2800 : 0.03860506095510637\n",
            "Accuracy after iteration 2800 : 98.59154929577466 %\n",
            "Loss after iteration 2900 : 0.03783072988945439\n",
            "Accuracy after iteration 2900 : 98.59154929577466 %\n",
            "Loss after iteration 3000 : 0.03707969613519486\n",
            "Accuracy after iteration 3000 : 98.59154929577466 %\n",
            "Loss after iteration 3100 : 0.03632682160703547\n",
            "Accuracy after iteration 3100 : 98.59154929577466 %\n",
            "Loss after iteration 3200 : 0.0355506330956233\n",
            "Accuracy after iteration 3200 : 98.59154929577466 %\n",
            "Loss after iteration 3300 : 0.03473935374780259\n",
            "Accuracy after iteration 3300 : 98.59154929577466 %\n",
            "Loss after iteration 3400 : 0.03389620942169328\n",
            "Accuracy after iteration 3400 : 98.59154929577466 %\n",
            "Loss after iteration 3500 : 0.03303175556382345\n",
            "Accuracy after iteration 3500 : 98.59154929577466 %\n",
            "Loss after iteration 3600 : 0.03215042022342466\n",
            "Accuracy after iteration 3600 : 98.59154929577466 %\n",
            "Loss after iteration 3700 : 0.03126495223621104\n",
            "Accuracy after iteration 3700 : 98.59154929577466 %\n",
            "Loss after iteration 3800 : 0.03037966868538511\n",
            "Accuracy after iteration 3800 : 98.59154929577466 %\n",
            "Loss after iteration 3900 : 0.029433833432479982\n",
            "Accuracy after iteration 3900 : 98.59154929577466 %\n",
            "Loss after iteration 4000 : 0.028404546358056483\n",
            "Accuracy after iteration 4000 : 98.59154929577466 %\n",
            "Loss after iteration 4100 : 0.02732679319854396\n",
            "Accuracy after iteration 4100 : 98.59154929577466 %\n",
            "Loss after iteration 4200 : 0.02630083801887529\n",
            "Accuracy after iteration 4200 : 98.59154929577466 %\n",
            "Loss after iteration 4300 : 0.025478067942549344\n",
            "Accuracy after iteration 4300 : 98.59154929577466 %\n",
            "Loss after iteration 4400 : 0.024869564031499456\n",
            "Accuracy after iteration 4400 : 98.59154929577466 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fbfd3ae8d30>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaFi7RU755r8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "995ce202-e56b-42de-fca6-266c36831ea1"
      },
      "source": [
        "plt.show()\n",
        "test = predict(model,X_test)\n",
        "test = pd.get_dummies(test)\n",
        "Y_test = pd.DataFrame(Y_test)\n",
        "print(\"Testing accuracy is: \",str(accuracy_score(Y_test, test) * 100)+\"%\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing accuracy is:  94.44444444444444%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9nkuTEg6APw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}